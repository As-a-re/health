#!/usr/bin/env python3
"""
Script to fine-tune a medical AI model for Akan language support
This would typically use datasets like AfriMed-QA and Masakhane
"""

import os
import torch
from transformers import (
    AutoTokenizer, AutoModelForQuestionAnswering,
    TrainingArguments, Trainer, DataCollatorWithPadding
)
from datasets import Dataset, load_dataset
import pandas as pd
from typing import Dict, List
import json
import asyncio
import sys
from motor.motor_asyncio import AsyncIOMotorClient
from datetime import datetime

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.core.config import settings

class AkanMedicalModelTrainer:
    def __init__(self, base_model: str = "bert-base-multilingual-cased"):
        self.base_model = base_model
        self.tokenizer = None
        self.model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
    def load_base_model(self):
        """Load the base multilingual model"""
        print(f"Loading base model: {self.base_model}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model)
        self.model = AutoModelForQuestionAnswering.from_pretrained(self.base_model)
        
        print(f"Model loaded on device: {self.device}")
        
    def tokenize_function(self, examples):
        """Tokenize the examples for training"""
        # Tokenize both the questions and contexts together
        tokenized = self.tokenizer(
            examples["question" if "question" in examples else "questions"],
            examples["context" if "context" in examples else "contexts"],
            truncation="only_second",
            max_length=384,
            stride=128,
            return_overflowing_tokens=True,
            return_offsets_mapping=True,
            padding="max_length"
        )
        
        # Since one example might give us several features, we need a map from a feature to
        # its corresponding example. This key gives us just that.
        sample_mapping = tokenized.pop("overflow_to_sample_mapping")
        offset_mapping = tokenized.pop("offset_mapping")
        
        # Let's label those examples!
        tokenized["start_positions"] = []
        tokenized["end_positions"] = []
        
        for i, offsets in enumerate(offset_mapping):
            # We will label impossible answers with the index of the CLS token.
            input_ids = tokenized["input_ids"][i]
            cls_index = input_ids.index(self.tokenizer.cls_token_id)
            
            # Grab the sequence corresponding to that example (to know what is the context and what is the question).
            sequence_ids = tokenized.sequence_ids(i)
            
            # One example can give several spans, this is the index of the example containing this span of text.
            sample_index = sample_mapping[i]
            answers = self.examples[sample_index]["answers"]
            
            # If no answers are given, set the cls_index as answer.
            if len(answers["answer_start"]) == 0:
                tokenized["start_positions"].append(cls_index)
                tokenized["end_positions"].append(cls_index)
            else:
                # Start/end character index of the answer in the text.
                start_char = answers["answer_start"][0]
                end_char = start_char + len(answers["text"][0])
                
                # Start token index of the current span in the text.
                token_start_index = 0
                while sequence_ids[token_start_index] != 1:  # 1 is the context part
                    token_start_index += 1
                
                # End token index of the current span in the text.
                token_end_index = len(input_ids) - 1
                while sequence_ids[token_end_index] != 1:
                    token_end_index -= 1
                
                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).
                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):
                    tokenized["start_positions"].append(cls_index)
                    tokenized["end_positions"].append(cls_index)
                else:
                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.
                    # Note: we could go after the last offset if the answer is the last word (edge case).
                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:
                        token_start_index += 1
                    tokenized["start_positions"].append(token_start_index - 1)
                    
                    while offsets[token_end_index][1] >= end_char:
                        token_end_index -= 1
                    tokenized["end_positions"].append(token_end_index + 1)
        
        return tokenized
        
    def train(self, output_dir: str = "akan_medical_model", num_train_epochs: int = 3):
        """
        Train the Akan medical model
        
        Args:
            output_dir: Directory to save the trained model
            num_train_epochs: Number of training epochs
        """
        print("Preparing training data...")
        dataset = self.prepare_akan_medical_data()
        
        # Convert to Hugging Face dataset
        train_dataset = Dataset.from_dict({
            "id": [str(i) for i in range(len(dataset))],
            "title": [""] * len(dataset),
            "context": [item["context"] for item in dataset],
            "question": [item["question"] for item in dataset],
            "answers": [{"text": item["answers"]["text"], 
                        "answer_start": item["answers"]["answer_start"]} 
                       for item in dataset]
        })
        
        # Tokenize the dataset
        print("Tokenizing dataset...")
        tokenized_datasets = train_dataset.map(
            self.tokenize_function,
            batched=True,
            remove_columns=train_dataset.column_names
        )
        
        # Define training arguments
        training_args = TrainingArguments(
            output_dir=output_dir,
            evaluation_strategy="no",
            learning_rate=2e-5,
            per_device_train_batch_size=8,
            per_device_eval_batch_size=8,
            num_train_epochs=num_train_epochs,
            weight_decay=0.01,
            push_to_hub=False,
            logging_dir='./logs',
            logging_steps=10,
            save_total_limit=3,
            save_strategy="epoch"
        )
        
        # Initialize trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=tokenized_datasets,
            tokenizer=self.tokenizer,
        )
        
        # Start training
        print("Starting model training...")
        trainer.train()
        
        # Save the model
        print(f"Saving model to {output_dir}")
        trainer.save_model(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        
        print("Training completed successfully!")
        return output_dir
    
    def prepare_akan_medical_data(self) -> list:
        """
        Prepare Akan medical training data from multiple sources
        Combines sample data with the medical knowledge base
        
        Returns:
            list: A list of training examples with context, question, and answers
        """
        # Sample Akan medical Q&A data
        sample_data = [
            # General health questions
            {
                "context": "Malaria yÉ› É”yare a É›fi asan a É›kÉ” so wÉ” É”sram no nipadua mu.",
                "question": "ÆdeÉ›n na É›ma É”yare atiridii ba?",
                "answers": {
                    "text": ["Asan a É›kÉ” so wÉ” É”sram no nipadua mu"],
                    "answer_start": [0]
                }
            },
            # Symptom questions
            {
                "context": "SÉ› wowo atiridii a, É›tumi de É”hyew, awu, ne ahÉ”hohia aba wo.",
                "question": "DeÉ›n ne atiridii no nsÉ›n?",
                "answers": {
                    "text": ["É”hyew, awu, ne ahÉ”hihia"],
                    "answer_start": [20]
                }
            },
            # Treatment questions
            {
                "context": "WÉ”de nnuru te sÉ› chloroquine, artemisinin-based combination therapies (ACTs) na É›sa atiridii.",
                "question": "DeÉ›n na wÉ”de sa atiridii?",
                "answers": {
                    "text": ["chloroquine, artemisinin-based combination therapies (ACTs)"],
                    "answer_start": [5]
                }
            },
            # Prevention questions
            {
                "context": "SÉ› wubetumi a, fa net a wÉ”de twa atiridii ho hyia, fa nnuru a É›kanyan atiridii, na fa nnebÉ”ne a É›kanyan atiridii a wÉ”de gu nsuo mu di dwuma.",
                "question": "Æte sÉ›n na mÉ›yÉ› a mebÉ›yÉ› mma atiridii annsa?",
                "answers": {
                    "text": ["fa net a wÉ”de twa atiridii ho hyia, fa nnuru a É›kanyan atiridii, na fa nnebÉ”ne a É›kanyan atiridii a wÉ”de gu nsuo mu di dwuma"],
                    "answer_start": [14]
                }
            },
            {
                "context": "Ti yare yÉ› É”yare a É›taa ba wÉ” stress, nsuo a wonnom, anaa É”haw foforÉ” nti. WÉ”tumi de home, nsuom, ne nnuro a É›tumi te yea so sa.",
                "question": "DÉ›n na É›de ti yare ba?",
                "answer": "stress, nsuo a wonnom, anaa É”haw foforÉ”",
                "answer_start": 45
            },
            {
                "context": "Æ†hyew yÉ› nipadua no É”kwan a É”fa so ko tia nyarewa. Nipadua mu hyew a É›yÉ› dÉ› yÉ› 98.6Â°F (37Â°C). Æ†hyew a É›boro 100.4Â°F (38Â°C) no na wÉ”frÉ› no É”hyew.",
                "question": "DÉ›n ne É”hyew a É›yÉ› dÉ›?",
                "answer": "98.6Â°F (37Â°C)",
                "answer_start": 85
            },
            {
                "context": "Æ†wa tumi yÉ› amoa anaa nea É›yÉ› nsuo. Ætumi fi nyarewa, allergy, asthma, anaa É”yare foforÉ” mu ba. Ayaresa gyina nea É›de ba no so.",
                "question": "DÉ›n na É›de É”wa ba?",
                "answer": "nyarewa, allergy, asthma, anaa É”yare foforÉ”",
                "answer_start": 65
            }
        ]
        
        # Convert to dataset format
        dataset = Dataset.from_list(sample_data)
        
        return dataset
    
    def tokenize_data(self, dataset: Dataset) -> Dataset:
        """Tokenize the dataset for training"""
        
        def tokenize_function(examples):
            # Tokenize questions and contexts
            tokenized = self.tokenizer(
                examples["question"],
                examples["context"],
                truncation=True,
                padding=True,
                max_length=512,
                return_offsets_mapping=True
            )
            
            # Find answer positions in tokenized text
            start_positions = []
            end_positions = []
            
            for i, (answer_start, answer) in enumerate(zip(examples["answer_start"], examples["answer"])):
                # Find token positions for answers
                offset_mapping = tokenized["offset_mapping"][i]
                
                # Find start token
                start_token = 0
                for idx, (start, end) in enumerate(offset_mapping):
                    if start <= answer_start < end:
                        start_token = idx
                        break
                
                # Find end token
                end_token = start_token
                answer_end = answer_start + len(answer)
                for idx, (start, end) in enumerate(offset_mapping[start_token:], start_token):
                    if start < answer_end <= end:
                        end_token = idx
                        break
                
                start_positions.append(start_token)
                end_positions.append(end_token)
            
            tokenized["start_positions"] = start_positions
            tokenized["end_positions"] = end_positions
            
            # Remove offset mapping as it's not needed for training
            del tokenized["offset_mapping"]
            
            return tokenized
        
        tokenized_dataset = dataset.map(tokenize_function, batched=True)
        
        return tokenized_dataset
    
    def train_model(self, train_dataset: Dataset, output_dir: str = "./akan_medical_model"):
        """Fine-tune the model on Akan medical data"""
        
        # Training arguments
        training_args = TrainingArguments(
            output_dir=output_dir,
            evaluation_strategy="no",  # No validation set in this example
            learning_rate=2e-5,
            per_device_train_batch_size=8,
            num_train_epochs=3,
            weight_decay=0.01,
            save_strategy="epoch",
            logging_dir=f"{output_dir}/logs",
            logging_steps=10,
            save_total_limit=2,
            load_best_model_at_end=False,
            dataloader_pin_memory=False,
        )
        
        # Data collator
        data_collator = DataCollatorWithPadding(
            tokenizer=self.tokenizer,
            padding=True
        )
        
        # Initialize trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
        )
        
        # Train the model
        print("Starting training...")
        trainer.train()
        
        # Save the model
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)
        
        print(f"Model saved to {output_dir}")
    
    def test_model(self, model_path: str):
        """Test the trained model"""
        
        # Load trained model
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForQuestionAnswering.from_pretrained(model_path)
        
        # Test questions
        test_cases = [
            {
                "context": "Ti yare yÉ› É”yare a É›taa ba wÉ” stress, nsuo a wonnom, anaa É”haw foforÉ” nti.",
                "question": "DÉ›n na É›de ti yare ba?"
            },
            {
                "context": "Æ†hyew yÉ› nipadua no É”kwan a É”fa so ko tia nyarewa.",
                "question": "DÉ›n ne É”hyew?"
            }
        ]
        
        print("\nTesting trained model:")
        for i, test_case in enumerate(test_cases, 1):
            inputs = tokenizer(
                test_case["question"],
                test_case["context"],
                return_tensors="pt",
                truncation=True,
                padding=True,
                max_length=512
            )
            
            with torch.no_grad():
                outputs = model(**inputs)
                start_scores = outputs.start_logits
                end_scores = outputs.end_logits
                
                # Get the most likely beginning and end of answer
                start_idx = torch.argmax(start_scores)
                end_idx = torch.argmax(end_scores)
                
                # Decode the answer
                answer_tokens = inputs["input_ids"][0][start_idx:end_idx+1]
                answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)
                
                print(f"Test {i}:")
                print(f"Question: {test_case['question']}")
                print(f"Answer: {answer}")
                print(f"Confidence: {torch.max(start_scores).item():.3f}")
                print()

# Enhanced medical knowledge base with Akan translations
MEDICAL_KNOWLEDGE_BASE = {
    "symptoms": {
        "headache": {
            "en": {
                "description": "Pain or discomfort in the head or neck area",
                "causes": ["stress", "dehydration", "lack of sleep", "eye strain", "tension"],
                "treatments": ["rest", "hydration", "pain relievers", "cold/warm compress"],
                "when_to_see_doctor": "If severe, persistent, or accompanied by fever, vision changes, or neck stiffness"
            },
            "ak": {
                "description": "Ti mu yea anaa É”haw a É›wÉ” ti anaa kÉ”n mu",
                "causes": ["adwennwen", "nsuo a wonnom", "nna a wonnya", "aniwa mu haw", "honam mu nkitahodi"],
                "treatments": ["home", "nom nsuo pii", "nnuro a É›tumi te yea so", "nsuonwini anaa hyew nneÉ›ma"],
                "when_to_see_doctor": "SÉ› É›yÉ› den, É›kÉ” so, anaa É”hyew, aniwa mu nsesa, anaa kÉ”n mu den ka ho a"
            }
        },
        "fever": {
            "en": {
                "description": "Elevated body temperature above normal (98.6Â°F/37Â°C)",
                "causes": ["infection", "inflammation", "heat exhaustion", "medication side effects"],
                "treatments": ["rest", "fluids", "fever reducers", "cool compress"],
                "when_to_see_doctor": "If temperature exceeds 103Â°F (39.4Â°C) or persists for more than 3 days"
            },
            "ak": {
                "description": "Nipadua mu hyew a É›boro dÉ› so (98.6Â°F/37Â°C)",
                "causes": ["nyarewa", "honam mu huru", "hyew mmoroso", "nnuro mu nsunsuanso"],
                "treatments": ["home", "nom nsuo pii", "nnuro a É›tumi te É”hyew so", "nsuonwini nneÉ›ma"],
                "when_to_see_doctor": "SÉ› hyew no boro 103Â°F (39.4Â°C) anaa É›kÉ” so nna mmiÉ›nsa a"
            }
        },
        "cough": {
            "en": {
                "description": "Sudden expulsion of air from the lungs",
                "causes": ["cold", "flu", "allergies", "asthma", "smoking"],
                "treatments": ["honey", "warm liquids", "throat lozenges", "humidifier"],
                "when_to_see_doctor": "If cough persists over 2 weeks, produces blood, or is accompanied by high fever"
            },
            "ak": {
                "description": "Mframa a É›firi amoa mu prÉ›ko pÉ›",
                "causes": ["awÉ”", "mframa yare", "allergy", "asthma", "tawa nom"],
                "treatments": ["É›wo", "nsuonwini nneÉ›ma", "menewa mu nnuro", "nsuo a É›yÉ› mframa"],
                "when_to_see_doctor": "SÉ› É”wa no kÉ” so nnawÉ”twe mmienu, mogya fi mu ba, anaa É”hyew kÉ›se ka ho a"
            }
        },
        "stomach_pain": {
            "en": {
                "description": "Discomfort or pain in the abdominal area",
                "causes": ["indigestion", "gas", "food poisoning", "ulcers", "appendicitis"],
                "treatments": ["rest", "bland foods", "hydration", "antacids"],
                "when_to_see_doctor": "If severe, persistent, or accompanied by vomiting, fever, or blood in stool"
            },
            "ak": {
                "description": "Yafunu mu yea anaa É”haw",
                "causes": ["aduane a woannyam yiye", "mframa", "aduane bÉ”ne", "yafunu mu kuru", "appendix yare"],
                "treatments": ["home", "aduane a É›nyÉ› den", "nom nsuo", "nnuro a É›tumi te yafunu mu yea so"],
                "when_to_see_doctor": "SÉ› É›yÉ› den, É›kÉ” so, anaa É”fe ka ho, É”hyew, anaa mogya wÉ” ayamde mu a"
            }
        },
        "diarrhea": {
            "en": {
                "description": "Loose, watery bowel movements",
                "causes": ["food poisoning", "viral infection", "bacterial infection", "medication"],
                "treatments": ["hydration", "BRAT diet", "probiotics", "rest"],
                "when_to_see_doctor": "If severe dehydration, blood in stool, or persists more than 3 days"
            },
            "ak": {
                "description": "Ayamde a É›yÉ› nsuo na É›yÉ› ntÉ›ntÉ›n",
                "causes": ["aduane bÉ”ne", "mframa nyarewa", "bacteria nyarewa", "nnuro"],
                "treatments": ["nom nsuo pii", "aduane a É›nyÉ› den", "bacteria pa", "home"],
                "when_to_see_doctor": "SÉ› nsuo a wonnom no sa, mogya wÉ” ayamde mu, anaa É›kÉ” so nna mmiÉ›nsa a"
            }
        }
    },
    "general_advice": {
        "en": {
            "emergency_signs": [
                "Difficulty breathing",
                "Chest pain",
                "Severe bleeding",
                "Loss of consciousness",
                "Severe allergic reaction"
            ],
            "prevention": [
                "Wash hands regularly",
                "Eat balanced diet",
                "Exercise regularly",
                "Get adequate sleep",
                "Stay hydrated"
            ]
        },
        "ak": {
            "emergency_signs": [
                "Home a É›yÉ› den",
                "Koko mu yea",
                "Mogya a É›sen pii",
                "Nyanim a É›yera",
                "Allergy a É›yÉ› den"
            ],
            "prevention": [
                "Hohoro wo nsa daa",
                "Di aduane a É›yÉ›",
                "YÉ› apÉ”muden dwuma daa",
                "Da yiye",
                "Nom nsuo pii"
            ]
        }
    }
}

async def setup_medical_knowledge():
    """Setup medical knowledge base in MongoDB"""
    print("Setting up medical knowledge base...")
    
    try:
        client = AsyncIOMotorClient(settings.MONGODB_URL)
        db = client.akan_health_db
        
        # Clear existing knowledge base
        await db.medical_knowledge.delete_many({})
        
        # Insert new knowledge base
        knowledge_doc = {
            "_id": "medical_knowledge_v1",
            "version": "1.0",
            "created_at": datetime.utcnow(),
            "data": MEDICAL_KNOWLEDGE_BASE
        }
        
        await db.medical_knowledge.insert_one(knowledge_doc)
        
        # Create search indexes
        await db.medical_knowledge.create_index([("data.symptoms", "text")])
        
        print("âœ… Medical knowledge base created")
        
        # Create sample health queries for testing
        sample_queries = [
            {
                "_id": "sample-query-1",
                "user_id": "test-user-id-456",
                "query_text": "I have a headache and feel tired",
                "query_language": "en",
                "response_text": "Headaches can be caused by stress, dehydration, lack of sleep, or eye strain. Try resting in a quiet, dark room, stay hydrated, and apply a cold compress. If the headache is severe or persistent, please consult a healthcare professional.",
                "response_language": "en",
                "confidence_score": "0.85",
                "model_used": "local_knowledge",
                "created_at": datetime.utcnow()
            },
            {
                "_id": "sample-query-2",
                "user_id": "akan-user-id-789",
                "query_text": "Me ti yea na me brÉ›",
                "query_language": "ak",
                "response_text": "Ti yare betumi afi stress, nsuo a wonnom, nna a wonnya, anaa aniwa mu haw mu ba. SÉ” hwÉ› sÉ› wobÉ›home wÉ” komm ne sum beae, nom nsuo pii, na fa nsuonwini nneÉ›ma to wo ti so. SÉ› ti yare no yÉ› den anaa É›kÉ” so a, kÉ” oduruyÉ›foÉ” nkyÉ›n.",
                "response_language": "ak",
                "confidence_score": "0.80",
                "model_used": "local_knowledge+akan",
                "created_at": datetime.utcnow()
            }
        ]
        
        for query in sample_queries:
            existing = await db.health_queries.find_one({"_id": query["_id"]})
            if not existing:
                await db.health_queries.insert_one(query)
                print(f"âœ… Created sample query: {query['_id']}")
        
        print("âœ… Medical knowledge base setup completed")
        
        client.close()
        
    except Exception as e:
        print(f"âŒ Medical knowledge setup failed: {e}")
        raise

async def main():
    """
    Main function to train the Akan medical model and set up the knowledge base
    """
    try:
        print("ðŸš€ Starting Akan Medical Model Training")
        print("=" * 50)
        
        # Initialize MongoDB connection
        print("\nðŸ”Œ Initializing database connection...")
        from app.core.database import init_db
        await init_db()
        
        # Setup medical knowledge base
        print("ðŸ“š Setting up medical knowledge base...")
        await setup_medical_knowledge()
        
        # Initialize the model trainer
        print("\nðŸ¤– Initializing Akan Medical Model Trainer...")
        trainer = AkanMedicalModelTrainer()
        
        # Load the base model
        print("ðŸ” Loading base multilingual model...")
        trainer.load_base_model()
        
        # Train the model
        print("\nðŸŽ“ Starting model training...")
        output_dir = trainer.train(
            output_dir="akan_medical_model",
            num_train_epochs=3  # Adjust based on your needs
        )
        
        print("\nâœ… Training completed successfully!")
        print(f"ðŸ“ Model saved to: {output_dir}")
        print("=" * 50)
        
    except Exception as e:
        print(f"\nâŒ An error occurred: {str(e)}")
        raise

if __name__ == "__main__":
    asyncio.run(main())
